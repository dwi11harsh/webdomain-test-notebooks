{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObLqy5uUS5Mr0jN3piHbNg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwi11harsh/webdomain-test-notebooks/blob/main/nextjs_coding_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4KBufJulGhM",
        "outputId": "38f67c9e-3c79-4118-d0cc-05f38bdc52a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/82.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain-openai langgraph deepagents tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "tXLA3kq0p3uK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "tavily_api_key = userdata.get(\"TAVILY_API_KEY\")\n",
        "\n",
        "if not openai_api_key or openai_api_key == \"\":\n",
        "  print(\"‚ùóÔ∏è openai api key not found\")\n",
        "else:\n",
        "  print(\"üöÄ openai api key is present for development\")\n",
        "\n",
        "if not tavily_api_key or tavily_api_key == \"\":\n",
        "  print(\"‚ùóÔ∏è tavily api key not found\")\n",
        "else:\n",
        "  print(\"üöÄ tavily api key is present for development\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B97zrSQPns-p",
        "outputId": "51284ebd-32ef-4457-9c2a-45826fccb3eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ openai api key is present for development\n",
            "üöÄ tavily api key is present for development\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from tavily import TavilyClient\n",
        "from deepagents import create_deep_agent\n",
        "from langchain.chat_models import init_chat_model"
      ],
      "metadata": {
        "id": "j6fz5ogaqeVL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = init_chat_model(\n",
        "    model = \"gpt-4o\",\n",
        "    model_provider=\"openai\",\n",
        "    api_key=openai_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "1mq8AYq20ufb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_client = TavilyClient(api_key=tavily_api_key)"
      ],
      "metadata": {
        "id": "LeFTjWmorexK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def internet_search(\n",
        "    query: str,\n",
        "    max_results: int = 5,\n",
        "    topic: Literal[\"general\", \"news\", \"finance\", \"coding\"] = \"general\",\n",
        "    include_raw_content: bool = False\n",
        "):\n",
        "  \"\"\"Run a web search\"\"\"\n",
        "  return tavily_client.search(\n",
        "      query,\n",
        "      search_depth=\"basic\",\n",
        "      max_results=max_results,\n",
        "      include_domains=None,\n",
        "      exclude_domains=None,\n",
        "      include_answer=False,\n",
        "      include_raw_content=False\n",
        "  )"
      ],
      "metadata": {
        "id": "dJfioKTLrpuF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "research_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report.\n",
        "\n",
        "You have access to an internet search tool as your primary means of gathering information.\n",
        "\n",
        "## `internet_search`\n",
        "\n",
        "Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bb8Nx-CMtIAg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_deep_agent(\n",
        "    model=model,\n",
        "    tools=[internet_search],\n",
        "    system_prompt=research_instructions,\n",
        "    middleware=None,\n",
        "    subagents=None,\n",
        "    response_format=None,\n",
        "    context_schema=None,\n",
        "    checkpointer=None,\n",
        "    store=None,\n",
        "    backend=None,\n",
        "    interrupt_on=None,\n",
        "    debug=False,\n",
        "    name=None,\n",
        "    cache=None\n",
        ")"
      ],
      "metadata": {
        "id": "RcGLMyyVtRM8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]})\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Q6uiVch5uWkf",
        "outputId": "a80562ad-83ff-4c12-c40a-b54ae426c92d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={}, id='9542c110-066f-44f0-823b-15fa4a78b986'),\n",
              "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 4509, 'total_tokens': 4529, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'id': 'chatcmpl-CcBE2TfRYfq1cc0T8G4TkcrMWGbqT', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--c82b24f2-e253-468c-964d-a58c9e558430-0', tool_calls=[{'name': 'internet_search', 'args': {'query': 'LangGraph', 'max_results': 5}, 'id': 'call_aTzjk6GkpgtTZpllTPHDcVPD', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4509, 'output_tokens': 20, 'total_tokens': 4529, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "  ToolMessage(content='{\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.langchain.com/langgraph\", \"title\": \"LangGraph\", \"content\": \"[Read a conceptual guide](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#human-in-the-loop) [See different agent architectures](https://docs.langchain.com/oss/python/langgraph/workflows-agents) [Learn about agent memory](https://docs.langchain.com/oss/python/langgraph/add-memory) Design agent-driven user experiences with LangGraph Platform\\'s APIs. Quickly deploy and scale your application with infrastructure built for agents. LangGraph sets the foundation for how we can build and scale AI workloads ‚Äî from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution to iterate quickly, debug immediately, and scale effortlessly.‚Äù LangGraph sets the foundation for how we can build and scale AI workloads ‚Äî from conversational agents, complex task automation, to custom LLM-backed experiences that \\'just work\\'.\", \"score\": 0.99972194, \"raw_content\": null}, {\"url\": \"https://github.com/langchain-ai/langgraph\", \"title\": \"langchain-ai/langgraph: Build resilient language agents as ...\", \"content\": \"[MIT license](/langchain-ai/langgraph/blob/main/LICENSE) # langchain-ai/langgraph Then, create an agent [using prebuilt components](https://langchain-ai.github.io/langgraph/agents/agents/): For more information, see the [Quickstart](https://langchain-ai.github.io/langgraph/agents/agents/). Or, to learn how to build an [agent workflow](https://langchain-ai.github.io/langgraph/concepts/low_level/) with a customizable architecture, long-term memory, and other complex task handling, see the [LangGraph basics tutorials](https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/). * [Comprehensive memory](https://langchain-ai.github.io/langgraph/concepts/memory/): Create truly stateful agents with both short-term working memory for ongoing reasoning and long-term persistent memory across sessions. * [Production-ready deployment](https://langchain-ai.github.io/langgraph/concepts/deployment_options/): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows. * [LangSmith Deployment](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/) ‚Äî Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. * [Examples](https://langchain-ai.github.io/langgraph/examples/): Guided examples on getting started with LangGraph.\", \"score\": 0.9993013, \"raw_content\": null}, {\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph?\", \"content\": \"*   [Overview](https://www.ibm.com/think/topics/ai-agents#7281535) *   [Overview](https://www.ibm.com/think/topics/components-of-ai-agents#498277090) *   [Learning](https://www.ibm.com/think/topics/ai-agent-learning#498277087) *   [Tutorial: LangGraph ReAct agent](https://www.ibm.com/think/tutorials/deploy-langgraph-react-agent-manage-it-support-tickets-watsonx-ai#1287801557) *   [Overview](https://www.ibm.com/think/topics/ai-agent-protocols#1509394340) *   [Tutorial: LangGraph ReAct agent](https://www.ibm.com/think/tutorials/deploy-langgraph-react-agent-manage-it-support-tickets-watsonx-ai#80364620) *   [Overview](https://www.ibm.com/think/insights/ai-agent-governance#1268897081) *   [Overview](https://www.ibm.com/think/topics/ai-agent-use-cases#257779831) *   [Human resources](https://www.ibm.com/think/topics/ai-agents-in-human-resources#257779835) LangGraph, created by [LangChain](https://www.ibm.com/think/topics/langchain), is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an [AI agent workflow](https://www.ibm.com/think/topics/ai-agents). LangGraph is also built on several key technologies, including [LangChain,](https://www.ibm.com/think/topics/langchain) a Python framework for building AI applications. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including [chatbots](https://www.ibm.com/think/topics/chatbots), state graphs and [other agent-based systems](https://www.ibm.com/think/topics/multiagent-system).\", \"score\": 0.99629277, \"raw_content\": null}, {\"url\": \"https://www.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/\", \"title\": \"Rejected for not using LangChain/LangGraph?\", \"content\": \"They asked about data movement in LangGraph specifically, and I responded that I don\\'t use LangGraph in production, then pivoted to model\", \"score\": 0.99390244, \"raw_content\": null}, {\"url\": \"https://docs.cloud.google.com/agent-builder/agent-engine/develop/langgraph\", \"title\": \"Develop a LangGraph agent | Vertex AI Agent Builder\", \"content\": \"LanggraphAgent(model = model, tools =[get_exchange_rate,# Optional (Python function) grounded_search_tool,# Optional (Grounding Tool) movie_search_tool,# Optional (Langchain Tool) generate_and_execute_code,# Optional (Vertex Extension)],) response = agent. LanggraphAgent(model = model, prompt = custom_prompt_template, checkpointer_kwargs = checkpointer_kwargs, checkpointer_builder = checkpointer_builder, tools =[get_exchange_rate],) response = agent. from  vertexai  import agent_engines def  lcel_builder(*, model,** kwargs): from  operator  import itemgetter from  langchain_core.prompts  import ChatPromptTemplate from  langchain_core.runnables  import RunnablePassthrough from  langchain_core.output_parsers  import StrOutputParser output_parser = StrOutputParser() planner = ChatPromptTemplate. from  vertexai  import agent_engines def  langgraph_builder(*, model,** kwargs): from  langchain_core.prompts  import ChatPromptTemplate from  langchain_core.output_parsers  import StrOutputParser from  langgraph.graph  import END, MessageGraph output_parser = StrOutputParser() planner = ChatPromptTemplate.\", \"score\": 0.9937588, \"raw_content\": null}], \"response_time\": 0.0, \"request_id\": \"93282f31-9191-4bda-814d-4654a82a7b0f\"}', name='internet_search', id='b9e48162-ff27-4b9c-bf47-8c4ab29576bc', tool_call_id='call_aTzjk6GkpgtTZpllTPHDcVPD'),\n",
              "  AIMessage(content='LangGraph is an open-source AI agent framework developed by LangChain, designed to build, deploy, and manage complex generative AI agent workflows. It uses a graph-based architecture to model and handle the intricate relationships between various components within an AI agent workflow. LangGraph aims to facilitate the design of agent-driven user experiences, allowing developers to deploy and scale applications with infrastructure suited for agents.\\n\\nLangGraph supports various functionalities, including conversational agents, complex task automation, and custom LLM-backed experiences. It provides comprehensive memory capabilities for building stateful agents, enabling both short-term working memory and long-term persistent memory. Additionally, the platform includes scalable infrastructure designed to handle long-running, stateful workflows confidently.\\n\\nOverall, it serves as a foundation for building and scaling AI workloads, offering prebuilt components, tutorials, and deployment platforms to support sophisticated agent systems effectively.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 171, 'prompt_tokens': 6071, 'total_tokens': 6242, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 4480}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b1442291a8', 'id': 'chatcmpl-CcBE5beseK7bScv3YMD7o6uvzhmUL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--e2aba502-494f-4e39-b296-0331844d213f-0', usage_metadata={'input_tokens': 6071, 'output_tokens': 171, 'total_tokens': 6242, 'input_token_details': {'audio': 0, 'cache_read': 4480}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}